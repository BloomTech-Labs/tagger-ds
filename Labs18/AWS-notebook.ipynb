{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the cell where we do all the imports\n",
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import pandas as pd\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.tokenize.regexp import regexp_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to an s3 bucket\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket='sagemaker-name'\n",
    "prefix = 'sagemaker/nlp-email'\n",
    "bucket_path = 'https://s3-{}.amazonaws.com/{}'.format(region,bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in the csv\n",
    "emails = pd.read_csv('name.csv')\n",
    "emails.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the columns we want\n",
    "emails = emails[['sender_email', 'Message', 'Subject', 'Tags', 'UID', 'first_tag']]\n",
    "emails.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure all the columns we want to be strings are strings\n",
    "emails['Message'] = emails['Message'].apply(str)\n",
    "emails['sender_email'] = emails['sender_email'].apply(str)\n",
    "emails['Subject'] = emails['Subject'].apply(str)\n",
    "emails['Tags'] = emails['Tags'].apply(str)\n",
    "emails['first_tag'] = emails['first_tag'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out all the unique tags\n",
    "emails['first_tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little feature engineering to spice things up\n",
    "emails['Text'] = emails['sender_email'] + ' ' + emails['Message'] + ' ' + emails['Subject']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emails[['First_Tag','Second_Tag']] = emails['Tags'].str.split(',', expand=True)\n",
    "# emails.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make like a banana and split\n",
    "train, test = train_test_split(emails, stratify=emails['first_tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2's a party and four is a crowd\n",
    "X_train = train['Text']\n",
    "y_train = train['first_tag']\n",
    "\n",
    "X_test = test['Text']\n",
    "y_test = test['first_tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that removes all unnecessary puncuation, html code, and/or any apostrophes lying around\n",
    "def clean_text(text):\n",
    "    # replace new line and carriage return with space\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace the numbers and punctuation (exclude single quote) with space\n",
    "    punc_list = '!\"#$%&()*+,-/:;<=>?[\\]^_{|}~' + '0123456789'\n",
    "    t = str.maketrans(dict.fromkeys(punc_list, \" \"))\n",
    "    text = text.translate(t)\n",
    "    \n",
    "    # replace single quote with empty character\n",
    "    t = str.maketrans(dict.fromkeys(\"''\", \"\"))\n",
    "    text = text.translate(t)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# joblib.dump(clean_text, 'clean_text.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# nltk's regexp tokenizer mixed with our personal clean_text function\n",
    "class tokenize:\n",
    "    def regnltk_tokenize(text):\n",
    "        text = clean_text(text)\n",
    "        words = regexp_tokenize(text, pattern = '\\s+', gaps = True)\n",
    "        return [lemmatizer.lemmatize(word) for word in words if (len(word) >= 3)]\n",
    "\n",
    "# joblib.dump(regnltk_tokenize, 'regnltk_tokenize.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim's stopwords mixed with a few I added\n",
    "my_stopwords = STOPWORDS.union(set(['jacobsohn', 'avraham', 'http', 'https', 'kalman', 'com', 'sdui', 'www']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the X's with the regex tokenize and my_stopwords\n",
    "tokeni_zer = tokenize\n",
    "vect = TfidfVectorizer(tokenizer=tokenize.regnltk_tokenize, stop_words=my_stopwords, min_df=0.02, max_df=0.98)\n",
    "X_train = vect.fit_transform(X_train)\n",
    "X_test = vect.transform(X_test)\n",
    "\n",
    "pickle.dump(vect, open('vect.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the y's so the computer understands\n",
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "pickle.dump(encoder, open('labeller.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(bootstrap=False, n_estimators=300, max_depth=110, min_samples_leaf=3, min_samples_split=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pickle.dump(model, open('randomforest.pkl', 'wb'))\n",
    "\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)\n",
    "str_preds = encoder.inverse_transform(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Predictions': preds,\n",
    "                  'String Predictions': str_preds})\n",
    "df['String Predictions'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tags = encoder.inverse_transform(y_test)\n",
    "test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_pre = pd.DataFrame({'Actual': test_tags,\n",
    "                       'Preds': preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_pre['Actual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['preds'] = str_preds\n",
    "test[test['preds'] == 'Travel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(test['sender_email'] == 'normstormin@gmail.com') & (test['first_tag'] == 'Travel') & (test['preds'] == 'Travel')]['Message'][10857]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['first_tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up for the GridSearchCV\n",
    "param_grid = {\n",
    "    'bootstrap': [False],\n",
    "    'max_depth': [7, 10, 110],\n",
    "    'min_samples_leaf': [3, 6, 9],\n",
    "    'min_samples_split': [10],\n",
    "    'n_estimators': [500]\n",
    "}\n",
    "\n",
    "# Base model\n",
    "rfc = RandomForestClassifier()\n",
    "\n",
    "# grid search time!\n",
    "grid_search = GridSearchCV(estimator=rfc, param_grid=param_grid, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best params', grid_search.best_params_)\n",
    "\n",
    "best_grid = grid_search.best_estimator_\n",
    "print('Best grid', best_grid)\n",
    "grid_accuracy = evaluate(best_grid, X_test, y_test.values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
